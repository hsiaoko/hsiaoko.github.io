---
title: "Phoenix: A Substrate for Resilient Distributed Graph Analytics"
collection: publications
permalink: /publication/2019-asplos
date: 2019-04-13
venue: 'ACM International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS)'
paperurl: 'https://doi.org/10.1145/3297858.3304056'
citation: 'Roshan Dathathri, Gurbinder Gill, Loc Hoang, Keshav Pingali, “Phoenix: A Substrate for Resilient Distributed Graph Analytics,” Proceedings of the 24th ACM International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS), April 2019 (to appear).'
---
[(Download publication here)](https://www.cs.utexas.edu/~roshan/Phoenix.pdf) 

### Abstract

This paper presents Phoenix, a communication and synchronization
substrate that implements a novel protocol for recovering from 
fail-stop faults when executing graph analytics applications on distributed-memory
machines. The standard recovery technique in this space is checkpointing,
which rolls back the state of the entire computation to a state that existed 
before the fault occurred. The insight behind Phoenix is that this is not necessary 
since it is sufficient to continue the computation from a state that will ultimately 
produce the correct result. We show that for graph analytics applications,
the necessary state adjustment can be specified easily by the application programmer 
using a thin API supported by the Phoenix system.

Phoenix has no observable overhead during fault-free execution,
and it is resilient to any number of faults while guaranteeing that the correct
answer will be produced at the end of the computation. This is in contrast to
other systems in this space which may either have overheads even during fault-free
execution or produce only approximate answers when faults occur during execution.

We incorporated Phoenix into D-Galois, the state-of-the-art distributed graph
analytics system, and evaluated it on two production clusters. 
Our evaluation shows that in the absence of faults, Phoenix
is ~24x faster than GraphX, which provides fault-tolerance using the 
Spark system. Phoenix also outperforms a checkpoint-restart
technique implemented in D-Galois: in fault-free execution, Phoenix has
no observable overhead, whereas the checkpoint-restart technique has 31% overhead.
In addition, Phoenix mostly outperforms checkpointing when
faults occur, particularly in the common case when only a small number of
hosts fail simultaneously.